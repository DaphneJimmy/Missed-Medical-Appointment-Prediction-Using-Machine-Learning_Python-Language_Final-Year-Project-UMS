import pandas as pd
      import numpy as np
      import matplotlib.pyplot as plt
      import matplotlib.colors as colors
      from sklearn.utils import resample
      from sklearn.model_selection import train_test_split
      from sklearn.preprocessing import scale
      from sklearn.svm import SVC
      from sklearn.model_selection import GridSearchCV
      from sklearn.metrics import confusion_matrix
      from sklearn.metrics import plot_confusion_matrix
      from sklearn.decomposition import PCA
      
df = pd.read_csv('No_show_issue.csv',
      header=1)
df.head()
 
      
df.drop('PatientId', axis=1, inplace=True) #set axis=0 to remove rows, axis=1 to remove columns
      df.head()
 
df.drop('AppointmentID', axis=1, inplace=True)
      df.head()

      
df.drop('ScheduledDay', axis=1, inplace=True)
      df.head()

df.drop('AppointmentDay', axis=1, inplace=True)
      df.head()

      
df.drop('Neighbourhood', axis=1, inplace=True)
      df.head()

      
df.dtypes #identify missing data

      
df['Gender'].unique()


df['Age'].unique()

             
df['Scholarship'].unique()


df['Hipertension'].unique()


df['Diabetes'].unique()


df['Alcoholism'].unique()


df['Handcap'].unique()


df['SMS_received'].unique()


df['No-show'].unique()


#dealing with missing data
      len(df.loc[(df['Age'] == 0) | (df['Scholarship'] == 0)])


len(df)


df_no_missing = df.loc[(df['Age'] !=0) & (df['Scholarship'] !=0)]
len(df_no_missing)


df_no_missing['Age'].unique()

             
df_no_missing['Scholarship'].unique()

#downsample the data
len(df_no_missing)


 df_missed = df_no_missing[df_no_missing['No-show'] == 1]
      df_not_missed = df_no_missing[df_no_missing['No-show'] == 2]
 df_missed_downsampled = resample(df_missed,
            replace=False,
            n_samples=100,
            random_state=42)
len(df_missed_downsampled)


df_not_missed_downsampled = resample(df_not_missed,
            replace=False,
            n_samples=100,
            random_state=42)
len(df_not_missed_downsampled)


df_downsample = pd.concat([df_missed_downsampled, df_not_missed_downsampled])
len(df_downsample)


#Format data: split data into dependent and independent variables
      x = df_downsample.drop('No-show', axis=1).copy() 
      x.head()

      
y = df_downsample['No-show'].copy()
      y.head()

      
 #Format the data: one-Hot Encoding
      pd.get_dummies(x, columns=['Gender']).head()

      
 x_encoded = pd.get_dummies(x, columns=['Gender',
                                        'Scholarship',
                                        'Hipertension',
                                        'Diabetes',
                                        'Alcoholism',
                                        'Handcap',
                                        'SMS_received'])
      x_encoded.head()

      
 x_train, x_test, y_train, y_test = train_test_split(x_encoded, y, random_state=42)
      x_train_scaled = scale(x_train)
      x_test_scaled = scale(x_test)
#Build a preliminary SVM
      clf_svm = SVC(random_state=42)
      clf_svm.fit(x_train_scaled, y_train)

 plot_confusion_matrix(clf_svm,
                       x_test_scaled,
                       y_test,
                       values_format='d',
                       display_labels=["Missed", 'Not Missed'])


 #Optimize Parameters with cross validation & GridSearchCV()
      param_grid = [
          {'C': [0.5, 1, 10, 100],  #value for C must be > 0
           'gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],
           'kernel': ['rbf']},
      ]
      #including C=1  and gamma='scale' as possible choices since they are default values.

      optimal_params = GridSearchCV(
              SVC(),
              param_grid,
              cv=5,
              scoring='accuracy',
              verbose=0  #if u want to see what Grid Search is doing, set verbose=2
          )

      optimal_params.fit(x_train_scaled, y_train)
      print(optimal_params.best_params_)


#Building, evaluating, drawing, and interpreting final SVM
clf_svm = SVC(random_state=42, C=0.5, gamma='scale')
clf_svm.fit(x_train_scaled, y_train)

plot_confusion_matrix(clf_svm,
                      x_test_scaled,
                      y_test,
                      values_format='d',
                      display_labels=["Missed", "Not Missed"])


len(df_downsample.columns)


pca = PCA() #By default, PCA() centers the data, but does not scale it
      x_train_pca = pca.fit_transform(x_train_scaled)

      per_var = np.round(pca.explained_variance_ratio_*100, decimals=1)
      labels = [str(x) for x in range(1, len(per_var)+1)]

      plt.bar(x=range(1,len(per_var)+1), height=per_var)
      plt.tick_params(
          axis='x',      #change apply to the x-axis
          which='both',  #both major and minor ticks are affected
          bottom=False,  #ticks along the bottom edge are off
          top=False,     #ticks along the top edge are off
          labelbottom=False)   #labels along the bottom edge are off
      plt.ylabel('Percentage of Explained Variance')
      plt.xlabel('Principle Components')
      plt.title('Scree Plot')
      plt.show()

 train_pc1_coords = x_train_pca[:, 0]
      train_pc2_coords = x_train_pca[:, 1]

      #pc1 contains x-axis coordinates of the data after PCA
      #pc2 contains y-axis cordinates of data after PCA

      #Now center and scale the PCs...
      pca_train_scaled = scale(np.column_stack((train_pc1_coords, train_pc2_coords)))

      #Now we optimize the SVM fit to the x and y-axis coordinates of the data after PCA dimension reduction
      param_grid = [
          {'C': [1, 10, 100, 1000],
          'gamma': ['scale', 1, 0.1, 0.01, 0.001, 0.0001],
          'kernel': ['rbf']},
      ]

      optimal_params = GridSearchCV(
              SVC(),
              param_grid,
              cv=5,
              scoring='accuracy',
              verbose=0
          )

      optimal_params.fit(pca_train_scaled, y_train)
      print(optimal_params.best_params_)


clf_svm = SVC(random_state=42, C=1, gamma=1)
clf_svm.fit(pca_train_scaled, y_train)

#Transform the test dataset with the PCA..
x_test_pca = pca.transform(x_train_scaled)
#x_test_pca = pca.transform(x_test_scaled)
test_pc1_coords = x_test_pca[:, 0]
test_pc2_coords = x_test_pca[:, 1]

#Now create a matrix of points that we use to show the decision regions
#The matrix will be a little bit larger that transformed PCA points so that we can plot all of
#the PCA points on it without them being on the edge
x_min = test_pc1_coords.min() - 1
x_max = test_pc1_coords.max() + 1

y_min = test_pc2_coords.min() - 1
y_max = test_pc2_coords.max() + 1

xx, yy = np.meshgrid(np.arange(start=x_min, stop=x_max, step=0.1),
      np.arange(start=y_min, stop=y_max, step=0.1))

#Now we will classify every point in that matrix with SVM point in that
#matrix with SVM. Points on one side of the classification boundary will get 0, and points on the other
#side will get 1.
z = clf_svm.predict(np.column_stack((xx.ravel(), yy.ravel())))
#Right now, z is just along array of lots of 0s and 1s, which
#reflect how each point in the mesh was classified.
#We use reshape() so that each classification (0 or 1) corresponds
#to a specific point in the matrix.
z = z.reshape(xx.shape)

fig, ax = plt.subplots(figsize=(10,10))
#Now we will use contour() to draw a filled contour plot using the matrix values and classifications.
#The contour will be filled according to the predicted classifications (0s and 1s) in z
ax.contourf(xx, yy, z, alpha=0.1)

#Now create custom colours for the actual data points
cmap = colors.ListedColormap(['#e41a1c', '#4daf4a'])
#Now draw the actual data points - these will be colored by their known (not predicted) classifications
scatter = ax.scatter(test_pc1_coords, test_pc2_coords, c=y_train,
      cmap=cmap,
      s=100,
      edgecolors='k', #'k'=black
      alpha=0.7)

#Now create a legend
legend = ax.legend(scatter.legend_elements()[0],
      scatter.legend_elements()[1],
            loc="upper right")
legend.get_texts()[0].set_text("Yes Missed")
legend.get_texts()[1].set_text("Not Missed")

#Now add axis labels and titles
ax.set_ylabel('PC2')
ax.set_xlabel('PC1')
ax.set_title('Decision surface using the PCA transformed/projected features')
#plt.savefig('svm_default.png')
plt.show()
